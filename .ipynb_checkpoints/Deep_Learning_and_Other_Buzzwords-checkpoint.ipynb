{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Off-the-Shelf Tools for Deep Learning, NLP, and Other Fun Buzzwords\n",
    "## Jeff Jacobs, Sept. 27, 2019\n",
    "![bert](./bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tools\n",
    "\n",
    "### Gensim (https://radimrehurek.com/gensim/)\n",
    "\n",
    "Originally a topic modelling library, BUT also really good for word embedding stuff (the main library I use)\n",
    "\n",
    "### spaCy (https://spacy.io/)\n",
    "\n",
    "Better than Gensim (imo) for \"standard\" NLP tasks: Part-of-Speech Tagging, Dependency Parsing, Named Entity Recognition\n",
    "\n",
    "### scikit-learn (https://scikit-learn.org/stable/)\n",
    "\n",
    "General machine learning library (so, can be used for any type of data: text, images, video, audio, etc.)\n",
    "\n",
    "### AllenNLP (https://allennlp.org/)\n",
    "\n",
    "Will (should) obviate all of the above in a few years: NLP library built on top of PyTorch general deep learning library (only real competition for PyTorch is Google TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "### Buzzwords\n",
    "\n",
    "* **Artificial Intelligence**: Figuring out how to do human things with computers\n",
    "* **Machine Learning**: A set of approaches/algorithms which aim to find (potenitally complex) patterns in data\n",
    "* **Supervised Marchine Learning**: Trying to find patterns in input data $X$ which do a good job at predicting output data $Y$. Typically, \"trained\" on 80% of full dataset and evaluated (tested) on 20%. In NLP, document classification is most prominent example.\n",
    "* **Unsupervised Machine Learning**: Trying to find patterns in input data $X$ full stop. For example, find clusters of data points. In NLP, topic modelling is most prominent example.\n",
    "* **Neural Network**: A machine learning algorithm which learns a mapping between input and output via a series of \"layers\" (matrix multiplications of inputs with a weight matrix to produce outputs) connected non-linearly in a network\n",
    "* **Deep Learning**: Machine learning with a neural network...\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* I want to label a set of documents: *Document Classification*\n",
    "* I want to find people/places/events/things mentioned in a set of documents: *Named Entity Recognition*\n",
    "* I want to get a sense of whether a set of documents is talking about a person/place/event/thing in a positive or negative light: *Sentiment Analysis*\n",
    "* I want to understand how discourse regarding a subject(s) changes over time: *Diachronic Word Embeddings*\n",
    "* Other buzzwords: *Language Modeling* (e.g., Text Generation), *Sequence-to-Sequence Learning* (e.g., Translation), *End-to-End Models* (e.g., Image Captioning), *Transfer Learning*: learn on domain $X$, apply knowledge to domain $Y$ (e.g., learn Van Gogh's artistic style, then paint this pic of my house in the style of Van Gogh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## This Talk\n",
    "\n",
    "*Document Classification*. BUT, the real moral is that the models discussed here are specifically intended to encode linguistic knowledge that will be helpful for *ANY* text-analytic task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History of Text Analysis in One Slide\n",
    "\n",
    "1. The olden days: *Feature Engineering*\n",
    "2. The enlightenment: *Automagically-Learned Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Deep Learning\"? \"Neural Network\"? \"Word Embeddings\"?\n",
    "\n",
    "### 3 birds with one stone: let's learn about Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pavlov's Robot\n",
    "\n",
    "\"The camera lens aperture is too small.\"\n",
    "\n",
    "| Target  | Highlighted | Context |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| the  | (_The_) **camera lens]]** aperture is too small. | {camera, lens} |\n",
    "| camera  | **The** (_camera_) **lens aperture]]** is too small. | {the, lens, aperture} |\n",
    "| lens | **[[The camera** (_lens_) **aperture is]]** too small. | {the, camera, aperture, is} |\n",
    "| aperture | The **[[camera lens** (_aperture_) **is too]]** small. | {camera, lens, is, too} |\n",
    "| is | The camera **[[lens aperture** (_is_) **too small]]**. | {lens, aperture, too, small} |\n",
    "| too | The camera lens **[[aperture is** (_too_) **small**. | {aperture, is, small} |\n",
    "| small | The camera lens aperture **[[is too** (_small_). | {is, too} |\n",
    "\n",
    "![w2v](w2v_modified.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And so you'll have two vectors:\n",
    "$$\n",
    "predicted(\\texttt{camera}) = \\begin{pmatrix}P(\\texttt{ant}) = 0.1 \\\\ P(\\texttt{aperture}) = 0.1 \\\\ P(\\texttt{barber}) = 0.003 \\\\ \\vdots \\\\ P(\\texttt{zoo}) = 0.05\\end{pmatrix}, \\; actual(\\texttt{camera}) = \\begin{pmatrix}P(\\texttt{ant}) = 0 \\\\ P(\\texttt{aperture}) = 0.333 \\\\ P(\\texttt{barber}) = 0 \\\\ \\vdots \\\\ P(\\texttt{zoo}) = 0\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How different are they?\n",
    "$$\n",
    "\\mathcal{L}(predicted, actual)\n",
    "$$\n",
    "In this case, Cross-Entropy Loss:\n",
    "$$\n",
    "-\\sum_{i=1}^N\\mathbb{1}[\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{blockarray}{ccccc}\n",
    "& & \\BAmulticolumn{3}{c}{Predicted} \\\\\n",
    "& & \\textsf{Worker} & \\textsf{Firm} & \\textsf{Other} \\\\\n",
    "\\begin{block}{cc(ccc)}\n",
    "\\multirow{3}{*}{\\rotatebox{90}{$Actual$}} & \\textsf{Worker} & $4$ & $2$ & $24$ \\\\\n",
    " & \\textsf{Firm} & $4$ & $5$ & $3$ \\\\\n",
    " & \\textsf{Other} & $0$ & $0$ & $23$ \\\\\n",
    "\\end{block}\n",
    "\\end{blockarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bordermatrix{\n",
    "           & f(e_1)  & \\dots & f(e_j)  & \\dots  & f(e_p)  \\cr\n",
    "    f_1    & a_{1,1} &       & a_{1,j} & \\dots  & a_{1,p} \\cr\n",
    "    f_2    & a_{2,1} &       & a_{2,j} & \\dots  & a_{2,p} \\cr\n",
    "    \\vdots & \\vdots  &       & \\vdots  & \\ddots & \\vdots  \\cr\n",
    "    f_n    & a_{n,1} & \\dots & a_{n,j} & \\dots  & a_{n,p} \\cr\n",
    "  }\n",
    "\\]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.summarization.textcleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pre-trained word embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your own word embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "blum_model = FastText(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blum_file = \"killing_hope.txt\"\n",
    "sentence_iter = gensim.models.word2vec.LineSentence(source=blum_file)\n",
    "blum_model.build_vocab(sentence_iter)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
