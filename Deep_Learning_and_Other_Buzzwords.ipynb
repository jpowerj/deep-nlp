{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Off-the-Shelf Tools for Deep Learning, NLP, and Other Fun Buzzwords\n",
    "## Jeff Jacobs, Sept. 27, 2019\n",
    "![bert](images/bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Intro\n",
    "    * Buzzwords\n",
    "    * NLP Tasks (More Buzzwords)\n",
    "    * NLP/Deep Learning/Data Science Tools\n",
    "    * How Do We Turn Text Into Numbers?\n",
    "* Tutorial: Deep Contextual Word Embeddings via BERT\n",
    "    * Word Embeddings: Rapid Overview\n",
    "    * \"Strong Baseline\": word2vec, GloVe, and fastText\n",
    "    * Going Deep with BERT\n",
    "    * From Words to Sentences to Documents\n",
    "* Conclusion\n",
    "    * How to Keep Up With the NLP Literature, If You Want To\n",
    "    * Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Buzzwords\n",
    "\n",
    "* **Artificial Intelligence**: Figuring out how to do human things with computers\n",
    "* **Machine Learning**: A set of approaches/algorithms which aim to find (potenitally complex) patterns in data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Buzzwords\n",
    "\n",
    "* **Supervised Marchine Learning**: Trying to find patterns in input data $X$ which do a good job at predicting output data $Y$. Typically, \"trained\" on 80% of full dataset and evaluated (tested) on 20%. In NLP, document classification is most prominent example.\n",
    "* **Unsupervised Machine Learning**: Trying to find patterns in input data $X$ full stop. For example, find clusters of data points. In NLP, topic modelling is most prominent example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Buzzwords\n",
    "\n",
    "* **Neural Network**: A machine learning algorithm which learns a mapping between input and output via a series of \"layers\" (matrix multiplications of inputs with a weight matrix to produce outputs) connected non-linearly in a network\n",
    "* **Deep Learning**: Machine learning with a neural network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Buzzwords\n",
    "\n",
    "* **Natural Language Processing (NLP)**: Any of the above applied specifically to *language* data, i.e, text. Thus sometimes called \"Compuatational Linguistics\", \"Text Analysis\", \"Text-as-Data\", etc... NLP is the term used in CS, which trickles out to the rest, so I'll stick with NLP. It can be further broken down into (the \"old school\" and \"new school\", respectively, to oversimplify a lot):\n",
    "    * **Statistical NLP**: Was pretty much the only NLP from the field's founding until fairly recently -- using statistical models of language to make inferences from linguistic data (aka text)\n",
    "    * **Neural NLP**: Though neural networks have been around since the 1950s, recently some absurdly huge breakthroughs have been made by applying them to problems in NLP, making this the current \"standard\" type of NLP (hence Stanford changing the name of its NLP class to \"Deep Learning for NLP\"... after I left\n",
    "    Within social science, the most popular NLP technologies are:\n",
    "    * **Topic Modeling**: Using statistical generative models (\"Probabilistic Graphical Models\") to infer a set of underlying semantic topics discussed frequently across a corpus\n",
    "    * **Word/Document Embeddings**: Using neural networks to generate geometrically-interpretable \"semantic spaces\" where words/documents with similar semantic content/meaning will be closer together, for various ways of defining \"semantic content\" (as well as distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "### NLP Tasks\n",
    "\n",
    "* **Document Classification**: I want to label a set of documents\n",
    "* **Named Entity Recognition**: I want to find people/places/events/things mentioned in a set of documents\n",
    "* **Sentiment Analysis**: I want to get a sense of whether a set of documents is talking about a person/place/event/thing in a positive or negative light\n",
    "* **Diachronic Word Embeddings**: I want to understand how discourse regarding a subject(s) changes over time\n",
    "* Other buzzwords: **Language Modeling** (e.g., Text Generation), **Sequence-to-Sequence Learning** (e.g., Translation), **End-to-End Models** (e.g., Image Captioning), **Transfer Learning**: learn on domain $X$, apply knowledge to domain $Y$ (e.g., learn Van Gogh's artistic style, then paint this pic of my house in the style of Van Gogh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The NLP Practitioner's Toolbox\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/gensim.png\" width=\"200px\"></td>\n",
    "        <td><img src=\"images/spacy.png\" width=\"200px\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/scikit_learn.png\" width=\"200px\"></td>\n",
    "        <td><img src=\"images/allennlp.png\" width=\"200px\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Tools\n",
    "\n",
    "### Gensim (https://radimrehurek.com/gensim/)\n",
    "\n",
    "Originally a topic modelling library, BUT also really good for word embedding stuff (the main library I use)\n",
    "\n",
    "### spaCy (https://spacy.io/)\n",
    "\n",
    "Better than Gensim (imo) for \"standard\" linguistic tasks: Part-of-Speech Tagging, Dependency Parsing, Named Entity Recognition\n",
    "\n",
    "### scikit-learn (https://scikit-learn.org/stable/)\n",
    "\n",
    "General machine learning library (so, can be used for any type of data: text, images, video, audio, etc.)\n",
    "\n",
    "### AllenNLP (https://allennlp.org/)\n",
    "\n",
    "Will (should) obviate all of the above in a few years: NLP library built on top of PyTorch general deep learning library (only real competition for PyTorch is Google TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Honorable Mentions -- Throw Them In Your Toolkit)\n",
    "\n",
    "### NLTK (Natural Language Toolkit) (https://www.nltk.org/) \n",
    "\n",
    "Simple word/sentence tokenizers and stopword lists for several common languages\n",
    "\n",
    "### Pandas (https://pandas.pydata.org/) with NumPy (https://numpy.org/)\n",
    "\n",
    "The go-to libraries for working with *numeric* data -- as in, any data that can be formatted into a data table (in Pandas, a `DataFrame`). Honestly, I use it so much I tend to unconsciously write\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "at the beginning of my Python files, even if I never end up using them.\n",
    "\n",
    "### Apache OpenOffice Calc (https://www.openoffice.org/product/calc.html)\n",
    "\n",
    "I STRONGLY RECOMMEND NOT USING EXCEL FOR NLP STUFF -- Excel doesn't (easily) support UTF-8 (Unicode) standard, OpenOffice does by default. Also open source, which is cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Honorable Mentions -- Throw Them In Your Toolkit)\n",
    "\n",
    "### Bokeh (https://bokeh.pydata.org/en/latest/index.html) or Seaborn (https://seaborn.pydata.org/)\n",
    "\n",
    "You'll see a lot of visualization tutorials using `matplotlib` -- Seaborn provides a more high-level interface to mpl that is far more natural for social science people used to Stata/R/`ggplot2`, while Bokeh is nice in that it's integrated with Pandas and NumPy (it's made by the same team as NumPy). I'm personally leaning towards Bokeh because I need interactivity in my plots/visualizations, but Seaborn may be an easier place to start, especially if you don't need interactive plots\n",
    "\n",
    "### PyTorch (https://pytorch.org/) or TensorFlow (https://www.tensorflow.org/)\n",
    "\n",
    "PyTorch is the deep learning library underlying AllenNLP, so if you want to understand the \"guts\" of the (often byzantine) deep NLP methods in AllenNLP, PyTorch is where to start. TensorFlow is Google's competitor to PyTorch, and the two pretty much corner the market for Python deep learning tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Talk/Tutorial\n",
    "\n",
    "Focus is on deep learning approaches to **Document Classification**. BUT, the real moral is that the models discussed here are specifically intended to encode linguistic knowledge that will be helpful for *ANY* text-analytic task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History of Text Analysis in One Slide\n",
    "\n",
    "1. The olden days: *Feature Engineering*\n",
    "\n",
    "|Raw Text|Feature 1: num_words|Feature 2: baby_count|Feature 3: has_emojis|\n",
    "|-|-                   |-                    |-\n",
    "|im baby|2|1|False|\n",
    "|DaBaby's major-label debut, *Baby on Baby*, received a favorable 7.7 review from Pitchfork.|13|3|False|\n",
    "|who else is BRAVE enough 2 say I ATTENDED Key West High School, Home of the Conchs, and I also PRAY FOR DEATH 😂 EVERY DAY! 😂|27|0|True|\n",
    "\n",
    "2. The [Deep] Enlightenment: *Automagically-Learned Features*\n",
    "\n",
    "![images/deep_features.png](images/deep_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Deep Learning\"? \"Neural Network\"? \"Word Embeddings\"?\n",
    "\n",
    "### 3 birds with one stone: let's learn about Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pavlov's Robot\n",
    "\n",
    "\"The camera lens aperture is too small.\"\n",
    "\n",
    "| Target  | Highlighted | Context |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| the  | (_The_) **camera lens]]** aperture is too small. | {camera, lens} |\n",
    "| camera  | **The** (_camera_) **lens aperture]]** is too small. | {the, lens, aperture} |\n",
    "| lens | **[[The camera** (_lens_) **aperture is]]** too small. | {the, camera, aperture, is} |\n",
    "| aperture | The **[[camera lens** (_aperture_) **is too]]** small. | {camera, lens, is, too} |\n",
    "| is | The camera **[[lens aperture** (_is_) **too small]]**. | {lens, aperture, too, small} |\n",
    "| too | The camera lens **[[aperture is** (_too_) **small**. | {aperture, is, small} |\n",
    "| small | The camera lens aperture **[[is too** (_small_). | {is, too} |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Humans See It\n",
    "\n",
    "![w2v](images/w2v_modified.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Computers See It\n",
    "\n",
    "And so you'll have two vectors based on the target word $w^* = \\texttt{camera}$ with respect to the sampled sentence:\n",
    "\n",
    "$$\n",
    "predicted(\\texttt{camera}) = \\begin{pmatrix}P(\\texttt{ant}) = 0.1 \\\\ P(\\texttt{aperture}) = 0.1 \\\\ P(\\texttt{barber}) = 0.003 \\\\ \\vdots \\\\ P(\\texttt{zoo}) = 0.05\\end{pmatrix}, \\; actual(\\texttt{camera}) = \\begin{pmatrix}P(\\texttt{ant}) = 0 \\\\ P(\\texttt{aperture}) = 0.333 \\\\ P(\\texttt{barber}) = 0 \\\\ \\vdots \\\\ P(\\texttt{zoo}) = 0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the vector on the right will be all zeros except for the slots corresponding to the $K \\leq 4$ words that are actually in the context of \"camera\" in the sampled sentence, and the non-zero entries will just be $1/K$ (reflecting that we're sampling uniformly from the $K$ words in the context), in this case $1/3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How'd We Do?\n",
    "\n",
    "We want to punish the network Pavlov-style in proportion to how badly it's doing, so we need a way to measure how badly it's doing, which we'll call the *loss function*:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(predicted, actual)\n",
    "$$\n",
    "\n",
    "Hand-waving over the details, we can use Cross-Entropy Loss $\\mathbb{H}(P, \\hat{P}) = \\mathbb{E}_P[-\\log(\\hat{P})]$, which basically measures how bad an *estimated* probability distribution $\\hat{P}$ is as an approximation of the *true* (population/non-estimated) probability distribution $P$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(predicted, actual) = -\\mathbb{H}(actual, predicted) = -\\sum_{w \\in V}^{N}actual(w)\\log(predicted(w)) = -\\sum_{w \\in context(w^*)}^{N}actual(w)\\log(predicted(w)),\n",
    "$$\n",
    "where $V$ is the vocabulary, the set of all words in the corpus, and where the final expression emphasizes that the only non-zero terms in the sum are the cross-entropy terms between the words that *actually* appear in the context of $w^*$ and the model's probability prediction for these words.\n",
    "\n",
    "Beyond that point, it's just math and more math. Tl;dr, it's just a giant application of the chain rule to compute gradients from the end of the neural network back to the beginning, and using these gradients to update the weights inside the individual neurons of the network. This process is called [*Backpropagation*](https://en.wikipedia.org/wiki/Backpropagation), if you want to search that term to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do We Do With These Vectors?\n",
    "\n",
    "(Gensim train-your-own-embeddings tutorial!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Second Generation\": Deep *Contextualized* Word Embeddings\n",
    "\n",
    "![imagenet_moment](images/imagenet_moment.png)\n",
    "(https://ruder.io/nlp-imagenet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![elmo_convo](images/elmo_convo.png)\n",
    "\n",
    "(http://jalammar.github.io/illustrated-bert/) (see also the BERT github: https://github.com/google-research/bert)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
